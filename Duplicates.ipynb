{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Duplicates.ipynb","provenance":[],"authorship_tag":"ABX9TyPgHzra6fh4u82iRtd5Fjmd"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9GkNcgAsXoSG","executionInfo":{"status":"ok","timestamp":1613706988605,"user_tz":480,"elapsed":22814,"user":{"displayName":"Srideep Dornala","photoUrl":"https://lh5.googleusercontent.com/-BTrIkV5kL3Q/AAAAAAAAAAI/AAAAAAAABf4/LTLFj7c1j3I/s64/photo.jpg","userId":"05790058063222680884"}},"outputId":"022f6483-53cd-464b-c20c-d531e76565fc"},"source":["# Necessary Import Statements\n","import pandas as pd\n","import numpy as np\n","\n","\n","BetaData = pd.read_csv('~/ASDRP/Data/BetaData.csv', low_memory=False)\n","data = pd.read_csv('~/ASDRP/Data/GammaData.csv', low_memory=False)\n","phl = pd.read_csv('~/ASDRP/Data/PHL_Dataset.csv', low_memory=False)\n","\n","print('should be 84 columns: ', len(BetaData.columns))\n","print('should be 92 columns: ', len(data.columns))\n","\n","numberDuplicates = 0\n","duplicates = pd.DataFrame(columns=list(data.columns))\n","deDupedData = pd.DataFrame(columns=list(data.columns))\n","badData = pd.DataFrame(columns=list(data.columns)) # To be filled in later\n","\n","# Checking if there is bad data, to find the source of the errors\n","areThereNulls = data['pl_pubdate'].isna().values.any()\n","print('Are there nulls?', areThereNulls)\n","\n","howManyNulls = data['pl_pubdate'].isna().sum()\n","print('How many?', howManyNulls)\n","\n","print('Size before: ', len(data['pl_name']))\n","data.dropna(how='any', subset=['pl_pubdate'], inplace=True)\n","print('Size after: ', len(data['pl_name']))\n","\n","# Removing bad data that can't be used in any way\n","print('Size before removing bad data:', len(data['pl_name']))\n","\n","for index, row in data.iterrows():\n","    date = data['pl_pubdate'][index]\n","    try:\n","        if len(date) < 8:\n","            data['pl_pubdate'][index] = date + '-01'\n","        data['pl_pubdate'][index] = pd.to_datetime(data['pl_pubdate'][index]) # This line is redundant, remove either this or line 56\n","    except:\n","#       badData.append(data.iloc[index])\n","        data.drop([index], inplace=True)\n","        data.reset_index(drop=True)\n","\n","# This is what I had before doing the try, except:\n","\n","    # for index, row in data.iterrows():\n","    #     date = data['pl_pubdate'][index]\n","    #     if '-00' in date or len(date) < 5 or date[4] != '-':\n","    # #        badData.append(data.iloc[index])\n","    #         data.drop([index], inplace=True)\n","    #         data.reset_index(drop=True)\n","    #     elif len(date) < 8:\n","    #         data['pl_pubdate'][index] = date + '-01'\n","    #\n","    # print('Number of bad data entries: (should be around 100?) ', len(badData['pl_name']))\n","\n","data['pl_pubdate'] = pd.to_datetime(data['pl_pubdate'])\n","\n","print('Size after removing bad data:', len(data['pl_name']))\n","\n","print(data['pl_pubdate'][0:10], ' ', type(data['pl_pubdate'][0:10]))\n","\n","\n","# Sorting the data\n","sortedData = data.sort_values(by=['pl_name', 'pl_pubdate'], ascending= [1,0], inplace=False, na_position='last')\n","\n","\n","# Store the duplicates before removing them\n","duplicates = pd.DataFrame(columns=list(data.columns))\n","mask = sortedData.duplicated(subset='pl_name', keep='first')\n","df_keep = sortedData.loc[~mask]\n","duplicates = duplicates.append(sortedData.loc[mask])\n","print('Duplicates size: ', len(duplicates['pl_name']))\n","\n","# Dropping the duplicates\n","print('Data size before:', len(sortedData['pl_name']))\n","deDupedData = sortedData.drop_duplicates(subset = 'pl_name', keep='first', inplace=False)\n","print('Data size after: ', len(deDupedData['pl_name']))\n","\n","# Here I'm calculating the number of duplicates\n","print(len(sortedData['pl_name']) - len(deDupedData['pl_name']))\n","numberDuplicates = len(duplicates['pl_name'])\n","print('Size of duplicates: (should equal the above number) ', numberDuplicates)\n","\n","print('Dataset with duplicates dropped: ', deDupedData[['pl_name', 'pl_pubdate']][0:10])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:22: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:23: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"],"name":"stderr"},{"output_type":"stream","text":["                   pl_name pl_pubdate\n","0                 11 Com b 2011-08-01\n","1                 11 UMi b 2017-03-01\n","2                 14 And b 2011-08-01\n","3                 14 Her b 2017-03-01\n","4               16 Cyg B b 2017-03-01\n","5                 18 Del b 2011-08-01\n","6  1RXS J160929.1-210524 b 2015-03-01\n","7                 24 Boo b 2018-08-01\n","8                 24 Sex b 2011-01-01\n","9                 24 Sex c 2011-01-01\n"],"name":"stdout"}]}]}